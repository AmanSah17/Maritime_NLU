# Maritime NLU - File Structure & Development Tasks

## ğŸ“ Complete Repository Structure

```
Maritime_NLU/
â”œâ”€â”€ README.md                          # Project overview & quick start
â”œâ”€â”€ PROJECT_SUMMARY.md                 # [NEW] Comprehensive project summary
â”œâ”€â”€ TECHNICAL_OPERATIONS.md            # [NEW] Detailed technical guide
â”œâ”€â”€ FILE_STRUCTURE_AND_TASKS.md        # [NEW] This file
â”‚
â”œâ”€â”€ notebook_01.ipynb                  # Exploratory NLU notebook
â”‚
â”œâ”€â”€ mnlu/                              # Python virtual environment
â”‚   â”œâ”€â”€ Include/
â”‚   â”œâ”€â”€ Lib/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â””â”€â”€ pyvenv.cfg
â”‚
â””â”€â”€ backend/
    â”œâ”€â”€ nlp_interpreter.py             # [LEGACY] Old NLU implementation
    â”œâ”€â”€ notebook_backend_02.ipynb      # Data import notebook (incomplete)
    â”‚
    â”œâ”€â”€ backend/                       # [LEGACY] Old backend folder
    â”‚   â”œâ”€â”€ maritime_data.db           # Database file
    â”‚   â””â”€â”€ [lock/log/err files]       # Database artifacts
    â”‚
    â””â”€â”€ nlu_chatbot/                   # â­ MAIN APPLICATION
        â”œâ”€â”€ requirements.txt           # Python dependencies
        â”œâ”€â”€ maritime_data.db           # Main SQLite database
        â”œâ”€â”€ maritime_data.db-shm       # SQLite WAL files
        â”œâ”€â”€ maritime_data.db-wal       # SQLite WAL files
        â”œâ”€â”€ maritime_sample_0104.db    # Sample database for testing
        â”‚
        â”œâ”€â”€ src/app/                   # â­ CORE BACKEND
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ main.py                # FastAPI server (273 lines)
        â”‚   â”œâ”€â”€ nlp_interpreter.py     # NLU parsing (358 lines)
        â”‚   â”œâ”€â”€ db_handler.py          # Database layer (215 lines)
        â”‚   â”œâ”€â”€ intent_executor.py     # Business logic (280 lines)
        â”‚   â”œâ”€â”€ main_query_engine.py   # [UNUSED] Alternative query engine
        â”‚   â””â”€â”€ run_e2e.py             # [UNUSED] E2E runner
        â”‚
        â”œâ”€â”€ frontend/                  # â­ STREAMLIT UI
        â”‚   â”œâ”€â”€ app.py                 # Main Streamlit app (233 lines)
        â”‚   â”œâ”€â”€ pages/                 # [EMPTY] Multi-page support
        â”‚   â”œâ”€â”€ package-lock.json      # [LEGACY] Node.js lock file
        â”‚   â””â”€â”€ __pycache__/
        â”‚
        â”œâ”€â”€ tests/                     # â­ TEST SUITE
        â”‚   â”œâ”€â”€ test_nlp_datetime.py   # NLU datetime tests (32 lines)
        â”‚   â”œâ”€â”€ test_nlu_and_db.py     # Integration tests (65 lines)
        â”‚   â”œâ”€â”€ test_champagne_cher.py # [UNUSED] Specific vessel test
        â”‚   â”œâ”€â”€ test_db.sqlite         # Test database
        â”‚   â””â”€â”€ __pycache__/
        â”‚
        â””â”€â”€ tools/                     # â­ UTILITY SCRIPTS
            â”œâ”€â”€ create_db_from_pkl.py          # Convert pickle â†’ SQLite (76 lines)
            â”œâ”€â”€ create_sample_db_from_pkl.py   # Create sample DB
            â”œâ”€â”€ import_pkls_to_db.py           # Batch import pickles
            â”œâ”€â”€ verify_db_import.py            # Validate import
            â”œâ”€â”€ benchmark_api.py               # Performance testing (33 lines)
            â”œâ”€â”€ e2e_runner.py                  # E2E test runner (47 lines)
            â”œâ”€â”€ start_uvicorn_with_db.bat      # Windows batch script
            â””â”€â”€ __pycache__/
```

---

## ğŸ“Š File Operations Summary

### Core Application Files (1,161 lines total)

| File | Lines | Purpose | Status |
|------|-------|---------|--------|
| `main.py` | 273 | FastAPI server, endpoints, job management | âœ… Active |
| `nlp_interpreter.py` | 358 | NLU parsing, intent/entity extraction | âœ… Active |
| `db_handler.py` | 215 | SQLite queries, pandas DataFrames | âœ… Active |
| `intent_executor.py` | 280 | SHOW/PREDICT/VERIFY business logic | âœ… Active |
| `frontend/app.py` | 233 | Streamlit UI, chat, mapping | âœ… Active |

### Test Files (97 lines total)

| File | Lines | Purpose | Status |
|------|-------|---------|--------|
| `test_nlp_datetime.py` | 32 | DateTime extraction tests | âœ… Active |
| `test_nlu_and_db.py` | 65 | Integration tests | âœ… Active |

### Tool Scripts (156 lines total)

| File | Lines | Purpose | Status |
|------|-------|---------|--------|
| `create_db_from_pkl.py` | 76 | Pickle â†’ SQLite conversion | âœ… Active |
| `benchmark_api.py` | 33 | API performance testing | âœ… Active |
| `e2e_runner.py` | 47 | End-to-end test runner | âœ… Active |

### Legacy/Unused Files

| File | Purpose | Status |
|------|---------|--------|
| `backend/nlp_interpreter.py` | Old NLU implementation | âš ï¸ Legacy |
| `main_query_engine.py` | Alternative query engine | âš ï¸ Unused |
| `run_e2e.py` | E2E runner (duplicate) | âš ï¸ Unused |
| `test_champagne_cher.py` | Specific vessel test | âš ï¸ Unused |
| `notebook_backend_02.ipynb` | Data import notebook | âš ï¸ Incomplete |

---

## ğŸ¯ Development Tasks & Roadmap

### Phase 1: Foundation (âœ… COMPLETE)
- [x] NLU interpreter with intent/entity extraction
- [x] SQLite database layer with pandas integration
- [x] Intent executor (SHOW/PREDICT/VERIFY)
- [x] FastAPI backend with REST endpoints
- [x] Streamlit frontend with mapping
- [x] Unit tests for NLU and DB
- [x] Data import tools

### Phase 2: Enhancement (â³ PLANNED)
- [ ] **Expand NLU Intents:**
  - [ ] ALERT intent (notify on anomalies)
  - [ ] COMPARE intent (compare two vessels)
  - [ ] ANALYZE intent (statistical analysis)
  - [ ] ROUTE intent (suggest optimal route)

- [ ] **Improve Matching:**
  - [ ] Phonetic matching (Soundex/Metaphone)
  - [ ] Abbreviation expansion (MSC â†’ Mediterranean Shipping Company)
  - [ ] Typo correction (Levenshtein distance)

- [ ] **Performance Optimization:**
  - [ ] Query result caching (Redis)
  - [ ] Batch query optimization
  - [ ] Database partitioning by date
  - [ ] Async query execution

- [ ] **Frontend Enhancements:**
  - [ ] Multi-vessel comparison view
  - [ ] Historical track animation
  - [ ] Real-time alerts dashboard
  - [ ] Export to CSV/GeoJSON

### Phase 3: ML/Prediction (â³ PLANNED)
- [ ] **LSTM Trajectory Prediction:**
  - [ ] Data preparation (sequence assembly)
  - [ ] Model training (encoder-decoder)
  - [ ] Model evaluation (Haversine error)
  - [ ] FastAPI `/predict` endpoint
  - [ ] Per-vessel model management

- [ ] **Anomaly Detection:**
  - [ ] Isolation Forest for outlier detection
  - [ ] Sudden course/speed changes
  - [ ] Geofence violations
  - [ ] Port dwell time analysis

### Phase 4: Production (â³ PLANNED)
- [ ] **Deployment:**
  - [ ] Docker containerization
  - [ ] Kubernetes orchestration
  - [ ] Cloud deployment (AWS/GCP/Azure)
  - [ ] CI/CD pipeline (GitHub Actions)

- [ ] **Monitoring & Logging:**
  - [ ] Structured logging (JSON)
  - [ ] Metrics collection (Prometheus)
  - [ ] Error tracking (Sentry)
  - [ ] Performance monitoring

- [ ] **Security:**
  - [ ] Authentication (OAuth2/JWT)
  - [ ] Authorization (role-based access)
  - [ ] API rate limiting
  - [ ] Data encryption (TLS)

- [ ] **Documentation:**
  - [ ] API documentation (Swagger/OpenAPI)
  - [ ] Architecture diagrams
  - [ ] Deployment guide
  - [ ] User manual

### Phase 5: Advanced Features (â³ FUTURE)
- [ ] **Real-time Processing:**
  - [ ] Kafka/RabbitMQ integration
  - [ ] Stream processing (Flink/Spark)
  - [ ] Live vessel tracking

- [ ] **Advanced Analytics:**
  - [ ] Port congestion analysis
  - [ ] Shipping lane optimization
  - [ ] Fuel consumption estimation
  - [ ] Emissions tracking

- [ ] **Integration:**
  - [ ] Weather API integration
  - [ ] Port information API
  - [ ] Vessel registry API
  - [ ] Compliance/sanctions checking

---

## ğŸ”§ Known Issues & TODOs

### Code Quality:
- [ ] Remove duplicate `return identifiers` in `nlp_interpreter.py` (line 357)
- [ ] Add missing `timedelta` import in `nlp_interpreter.py` (used in line 226)
- [ ] Consolidate legacy files (backend/nlp_interpreter.py)
- [ ] Add type hints to all functions
- [ ] Add docstrings to all classes/methods

### Testing:
- [ ] Increase test coverage (currently ~30%)
- [ ] Add integration tests for all endpoints
- [ ] Add performance benchmarks
- [ ] Add stress testing

### Documentation:
- [ ] Add API documentation (Swagger)
- [ ] Add architecture diagrams
- [ ] Add deployment guide
- [ ] Add troubleshooting guide

### Performance:
- [ ] Optimize datetime parsing (currently slow)
- [ ] Add query result caching
- [ ] Optimize fuzzy matching
- [ ] Profile memory usage

---

## ğŸš€ Quick Development Commands

### Setup:
```bash
cd backend\nlu_chatbot
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### Run Backend:
```bash
cd src\app
uvicorn main:app --reload --port 8000
```

### Run Frontend:
```bash
cd frontend
streamlit run app.py
```

### Run Tests:
```bash
$env:PYTHONPATH='src'
python -m pytest -v
```

### Run Benchmarks:
```bash
python tools\e2e_runner.py
```

### Create DB from Pickle:
```bash
python tools\create_db_from_pkl.py --pkl path\to\AIS_2020_01_03.pkl --out maritime_data.db
```

---

## ğŸ“‹ Configuration Files

### requirements.txt
```
fastapi
uvicorn
streamlit
pandas
numpy
folium
streamlit-folium
rapidfuzz
streamlit-aggrid
```

### Environment Variables
- `BACKEND_DB_PATH` - Override default database path
- `PYTHONPATH` - Set to `src` for test imports

### Database Configuration
- Default: `backend/nlu_chatbot/maritime_data.db`
- WAL mode enabled for concurrent reads
- Indexes on MMSI and BaseDateTime

